# Architecture: Machine Translation

## Model: MarianMT
```
Source Text → Encoder → Decoder → Target Text
```

## Transformer Architecture
- 6 encoder layers
- 6 decoder layers
- Multi-head attention
- 74M parameters

## Training
- Parallel corpora
- Back-translation
- Domain adaptation

## Next Steps
→ Project 28: Conversational AI with DialoGPT (117M)
