# Architecture: Domain Adaptation

## Process
```
RoBERTa-base → Domain Pre-training → Task Fine-tuning → Automotive Model
```

## Adaptation Steps
1. Collect domain corpus
2. Continued pre-training (MLM)
3. Task-specific fine-tuning
4. Evaluation

## Benefits
- Better domain understanding
- Improved accuracy
- Specialized vocabulary

## Next Steps
→ Project 32: Few-Shot Learning with GPT-3.5/4 (175B+)
